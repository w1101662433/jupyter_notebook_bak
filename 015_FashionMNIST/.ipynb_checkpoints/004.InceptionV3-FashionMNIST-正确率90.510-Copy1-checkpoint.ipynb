{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67d42871-45a0-45dd-8359-3c2e159ed5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f5b7916-8ab4-4aa7-8f90-9404bdeff545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python39\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "generator = torch.Generator()\n",
    "\n",
    "# 设置随机种子，确保实验可重复性\n",
    "torch.random.manual_seed(420)\n",
    "torch.cuda.manual_seed(420)\n",
    "generator.manual_seed(420)\n",
    "np.random.seed(420)\n",
    "random.seed(420)\n",
    "\n",
    "# 数据预处理，将FashionMNIST的28x28图像调整到299x299\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(299),  # 缩放图像使其变为299x299\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 归一化处理\n",
    "])\n",
    "\n",
    "\n",
    "# 从\"./dataset/\"目录加载FashionMNIST数据集，如果没有则会自动下载。\n",
    "train_data = FashionMNIST(root='./dataset/', train=True,  download=True,transform=transform)\n",
    "test_data = FashionMNIST(root='./dataset/', train=False,  download=True,transform=transform)\n",
    "train_batch = DataLoader(dataset=train_data, batch_size=32,  shuffle=True, num_workers=0, drop_last=False, generator=generator)\n",
    "test_batch = DataLoader(dataset=test_data, batch_size=32,  shuffle=False, num_workers=0, drop_last=False, generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a9e496f-a8f6-4a3c-a85d-d4c84e6d30c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import Inception3\n",
    "\n",
    "class BasicConv2d(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        super(BasicConv2d, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return F.relu(x, inplace=True)\n",
    "\n",
    "# 实现GoogleNet模型\n",
    "class Model(Inception3):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(Model, self).__init__()\n",
    "        self.Conv2d_1a_3x3 = BasicConv2d(1, 32, kernel_size=3, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # N x 3 x 299 x 299\n",
    "        x = self.Conv2d_1a_3x3(x)\n",
    "        # N x 32 x 149 x 149\n",
    "        x = self.Conv2d_2a_3x3(x)\n",
    "        # N x 32 x 147 x 147\n",
    "        x = self.Conv2d_2b_3x3(x)\n",
    "        # N x 64 x 147 x 147\n",
    "        x = self.maxpool1(x)\n",
    "        # N x 64 x 73 x 73\n",
    "        x = self.Conv2d_3b_1x1(x)\n",
    "        # N x 80 x 73 x 73\n",
    "        x = self.Conv2d_4a_3x3(x)\n",
    "        # N x 192 x 71 x 71\n",
    "        x = self.maxpool2(x)\n",
    "        # N x 192 x 35 x 35\n",
    "        x = self.Mixed_5b(x)\n",
    "        # N x 256 x 35 x 35\n",
    "        x = self.Mixed_5c(x)\n",
    "        # N x 288 x 35 x 35\n",
    "        x = self.Mixed_5d(x)\n",
    "        # N x 288 x 35 x 35\n",
    "        x = self.Mixed_6a(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6b(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6c(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6d(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_6e(x)\n",
    "        # N x 768 x 17 x 17\n",
    "       \n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.Mixed_7a(x)\n",
    "        # N x 1280 x 8 x 8\n",
    "        x = self.Mixed_7b(x)\n",
    "        # N x 2048 x 8 x 8\n",
    "        x = self.Mixed_7c(x)\n",
    "        # N x 2048 x 8 x 8\n",
    "        # Adaptive average pooling\n",
    "        x = self.avgpool(x)\n",
    "        # N x 2048 x 1 x 1\n",
    "        x = self.dropout(x)\n",
    "        # N x 2048 x 1 x 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 2048\n",
    "        x = self.fc(x)\n",
    "        # N x 1000 (num_classes)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0393784b-b507-47bd-b727-b9055961bbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\inception.py:80: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
      "  warnings.warn('The default weight initialization of inception_v3 will be changed in future releases of '\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# 初始化一个模型，输入图片通道数为1，输出特征为10\n",
    "model = Model(num_classes=10).to(device)\n",
    "# 使用负对数似然损失函数\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# 初始化Adam优化器，设定学习率为0.005\n",
    "opt = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bba9757-78a7-440f-8c47-88cc05f8ec2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 10.718929290771484 accuracy: 0.0\n",
      "10 loss: 3.2805380821228027 accuracy: 37.5\n",
      "20 loss: 1.7815992832183838 accuracy: 43.75\n",
      "30 loss: 1.8455778360366821 accuracy: 53.125\n",
      "40 loss: 1.4085487127304077 accuracy: 62.5\n",
      "50 loss: 1.472427248954773 accuracy: 56.25\n",
      "60 loss: 1.7953808307647705 accuracy: 46.875\n",
      "70 loss: 1.1278141736984253 accuracy: 68.75\n",
      "80 loss: 1.4390803575515747 accuracy: 56.25\n",
      "90 loss: 0.8133876919746399 accuracy: 68.75\n",
      "100 loss: 1.5607022047042847 accuracy: 65.625\n",
      "110 loss: 0.888356626033783 accuracy: 71.875\n",
      "120 loss: 0.8314771056175232 accuracy: 62.5\n",
      "130 loss: 1.2128280401229858 accuracy: 68.75\n",
      "140 loss: 0.7416073083877563 accuracy: 71.875\n",
      "150 loss: 0.561935305595398 accuracy: 78.125\n",
      "160 loss: 0.6898165345191956 accuracy: 68.75\n",
      "170 loss: 0.6605976223945618 accuracy: 87.5\n",
      "180 loss: 0.7485594153404236 accuracy: 68.75\n",
      "190 loss: 0.8794205784797668 accuracy: 62.5\n",
      "200 loss: 0.5030383467674255 accuracy: 84.375\n",
      "210 loss: 0.9778007864952087 accuracy: 56.25\n",
      "220 loss: 0.6338543891906738 accuracy: 78.125\n",
      "230 loss: 0.815418541431427 accuracy: 75.0\n",
      "240 loss: 1.6205885410308838 accuracy: 68.75\n",
      "250 loss: 1.0525157451629639 accuracy: 68.75\n",
      "260 loss: 0.8830358982086182 accuracy: 68.75\n",
      "270 loss: 0.44670045375823975 accuracy: 84.375\n",
      "280 loss: 0.869880199432373 accuracy: 65.625\n",
      "290 loss: 0.656927764415741 accuracy: 78.125\n",
      "300 loss: 0.5828741192817688 accuracy: 81.25\n",
      "310 loss: 0.4875711500644684 accuracy: 81.25\n",
      "320 loss: 0.23616205155849457 accuracy: 87.5\n",
      "330 loss: 0.46645233035087585 accuracy: 84.375\n",
      "340 loss: 0.7226375341415405 accuracy: 71.875\n",
      "350 loss: 0.7850490212440491 accuracy: 75.0\n",
      "360 loss: 0.41422510147094727 accuracy: 84.375\n",
      "370 loss: 0.5371692776679993 accuracy: 78.125\n",
      "380 loss: 0.32285642623901367 accuracy: 87.5\n",
      "390 loss: 0.6843578219413757 accuracy: 78.125\n",
      "400 loss: 0.485161691904068 accuracy: 78.125\n",
      "410 loss: 0.32736122608184814 accuracy: 93.75\n",
      "420 loss: 0.4913543462753296 accuracy: 81.25\n",
      "430 loss: 0.3371710777282715 accuracy: 87.5\n",
      "440 loss: 0.37093737721443176 accuracy: 87.5\n",
      "450 loss: 0.42536085844039917 accuracy: 87.5\n",
      "460 loss: 0.813785195350647 accuracy: 59.375\n",
      "470 loss: 0.9502010345458984 accuracy: 68.75\n",
      "480 loss: 0.6478977203369141 accuracy: 81.25\n",
      "490 loss: 0.47759440541267395 accuracy: 84.375\n",
      "500 loss: 0.6521520614624023 accuracy: 81.25\n",
      "510 loss: 0.7071319818496704 accuracy: 68.75\n",
      "520 loss: 0.3234264552593231 accuracy: 90.625\n",
      "530 loss: 0.39409390091896057 accuracy: 90.625\n",
      "540 loss: 0.7280227541923523 accuracy: 71.875\n",
      "550 loss: 0.7239362001419067 accuracy: 75.0\n",
      "560 loss: 0.346878319978714 accuracy: 81.25\n",
      "570 loss: 0.8302475214004517 accuracy: 75.0\n",
      "580 loss: 0.2448548525571823 accuracy: 93.75\n",
      "590 loss: 0.6004793047904968 accuracy: 78.125\n",
      "600 loss: 0.2515665292739868 accuracy: 93.75\n",
      "610 loss: 0.8209987282752991 accuracy: 68.75\n",
      "620 loss: 0.44863808155059814 accuracy: 84.375\n",
      "630 loss: 0.3472347855567932 accuracy: 93.75\n",
      "640 loss: 0.5558234453201294 accuracy: 75.0\n",
      "650 loss: 0.45720356702804565 accuracy: 84.375\n",
      "660 loss: 0.31830987334251404 accuracy: 87.5\n",
      "670 loss: 0.7906680107116699 accuracy: 65.625\n",
      "680 loss: 0.44730013608932495 accuracy: 84.375\n",
      "690 loss: 0.2289583683013916 accuracy: 96.875\n",
      "700 loss: 0.4048266112804413 accuracy: 84.375\n",
      "710 loss: 0.4461442232131958 accuracy: 81.25\n",
      "720 loss: 0.5609617233276367 accuracy: 84.375\n",
      "730 loss: 0.1900424063205719 accuracy: 93.75\n",
      "740 loss: 0.8347669243812561 accuracy: 75.0\n",
      "750 loss: 0.8507426977157593 accuracy: 71.875\n",
      "760 loss: 0.36372116208076477 accuracy: 84.375\n",
      "770 loss: 0.5529983639717102 accuracy: 71.875\n",
      "780 loss: 0.32169902324676514 accuracy: 84.375\n",
      "790 loss: 0.4161892235279083 accuracy: 81.25\n",
      "800 loss: 0.6087404489517212 accuracy: 81.25\n",
      "810 loss: 0.30948424339294434 accuracy: 90.625\n",
      "820 loss: 0.4299377501010895 accuracy: 78.125\n",
      "830 loss: 0.21758119761943817 accuracy: 93.75\n",
      "840 loss: 0.5154461860656738 accuracy: 71.875\n",
      "850 loss: 0.30394247174263 accuracy: 87.5\n",
      "860 loss: 0.39294493198394775 accuracy: 84.375\n",
      "870 loss: 0.36498358845710754 accuracy: 84.375\n",
      "880 loss: 0.4652235805988312 accuracy: 87.5\n",
      "890 loss: 0.5215740203857422 accuracy: 81.25\n",
      "900 loss: 0.5299127101898193 accuracy: 81.25\n",
      "910 loss: 0.3932739198207855 accuracy: 87.5\n",
      "920 loss: 0.39318355917930603 accuracy: 81.25\n",
      "930 loss: 0.4327661395072937 accuracy: 84.375\n",
      "940 loss: 0.2676964998245239 accuracy: 84.375\n",
      "950 loss: 0.23304465413093567 accuracy: 90.625\n",
      "960 loss: 0.15926732122898102 accuracy: 93.75\n",
      "970 loss: 0.5521286129951477 accuracy: 84.375\n",
      "980 loss: 0.22558023035526276 accuracy: 90.625\n",
      "990 loss: 0.4769127368927002 accuracy: 81.25\n",
      "1000 loss: 0.6108460426330566 accuracy: 78.125\n",
      "1010 loss: 0.540770411491394 accuracy: 87.5\n",
      "1020 loss: 0.4118083119392395 accuracy: 84.375\n",
      "1030 loss: 0.47194379568099976 accuracy: 78.125\n",
      "1040 loss: 0.6687501072883606 accuracy: 87.5\n",
      "1050 loss: 0.6211897134780884 accuracy: 75.0\n",
      "1060 loss: 0.5395328402519226 accuracy: 78.125\n",
      "1070 loss: 0.4157249927520752 accuracy: 87.5\n",
      "1080 loss: 0.5315762758255005 accuracy: 81.25\n",
      "1090 loss: 0.26089736819267273 accuracy: 90.625\n",
      "1100 loss: 0.632587730884552 accuracy: 75.0\n",
      "1110 loss: 0.4177064299583435 accuracy: 87.5\n",
      "1120 loss: 0.28087615966796875 accuracy: 87.5\n",
      "1130 loss: 0.40534693002700806 accuracy: 84.375\n",
      "1140 loss: 0.5590075850486755 accuracy: 78.125\n",
      "1150 loss: 0.2246386557817459 accuracy: 93.75\n",
      "1160 loss: 0.4090096056461334 accuracy: 87.5\n",
      "1170 loss: 0.7545170783996582 accuracy: 78.125\n",
      "1180 loss: 0.6174204349517822 accuracy: 75.0\n",
      "1190 loss: 0.7772636413574219 accuracy: 65.625\n",
      "1200 loss: 0.5873908996582031 accuracy: 78.125\n",
      "1210 loss: 0.24948933720588684 accuracy: 93.75\n",
      "1220 loss: 0.5704794526100159 accuracy: 71.875\n",
      "1230 loss: 0.3779136836528778 accuracy: 84.375\n",
      "1240 loss: 0.08185265213251114 accuracy: 96.875\n",
      "1250 loss: 0.5391752123832703 accuracy: 81.25\n",
      "1260 loss: 1.0132931470870972 accuracy: 75.0\n",
      "1270 loss: 0.29838183522224426 accuracy: 90.625\n",
      "1280 loss: 0.324405312538147 accuracy: 90.625\n",
      "1290 loss: 0.41428202390670776 accuracy: 87.5\n",
      "1300 loss: 0.2781919240951538 accuracy: 87.5\n",
      "1310 loss: 0.5209704637527466 accuracy: 81.25\n",
      "1320 loss: 0.31966227293014526 accuracy: 84.375\n",
      "1330 loss: 0.19008316099643707 accuracy: 93.75\n",
      "1340 loss: 0.3482828140258789 accuracy: 87.5\n",
      "1350 loss: 0.31070879101753235 accuracy: 87.5\n",
      "1360 loss: 0.38462573289871216 accuracy: 81.25\n",
      "1370 loss: 0.6282146573066711 accuracy: 75.0\n",
      "1380 loss: 0.26808032393455505 accuracy: 90.625\n",
      "1390 loss: 0.5452591180801392 accuracy: 84.375\n",
      "1400 loss: 0.5603626370429993 accuracy: 81.25\n",
      "1410 loss: 0.5930046439170837 accuracy: 84.375\n",
      "1420 loss: 0.17462599277496338 accuracy: 93.75\n",
      "1430 loss: 0.7733526825904846 accuracy: 75.0\n",
      "1440 loss: 0.6929094791412354 accuracy: 78.125\n",
      "1450 loss: 0.26582616567611694 accuracy: 93.75\n",
      "1460 loss: 0.4032672047615051 accuracy: 90.625\n",
      "1470 loss: 0.44058293104171753 accuracy: 81.25\n",
      "1480 loss: 0.21913442015647888 accuracy: 90.625\n",
      "1490 loss: 0.25401636958122253 accuracy: 93.75\n",
      "1500 loss: 0.38401490449905396 accuracy: 84.375\n",
      "1510 loss: 0.7050743103027344 accuracy: 84.375\n",
      "1520 loss: 0.7128006815910339 accuracy: 78.125\n",
      "1530 loss: 0.1871420294046402 accuracy: 96.875\n",
      "1540 loss: 0.3828245997428894 accuracy: 87.5\n",
      "1550 loss: 0.20816746354103088 accuracy: 93.75\n",
      "1560 loss: 0.3679566979408264 accuracy: 84.375\n",
      "1570 loss: 0.4374062418937683 accuracy: 84.375\n",
      "1580 loss: 0.7731670141220093 accuracy: 65.625\n",
      "1590 loss: 0.36983489990234375 accuracy: 81.25\n",
      "1600 loss: 0.2834923267364502 accuracy: 84.375\n",
      "1610 loss: 0.3177509009838104 accuracy: 93.75\n",
      "1620 loss: 0.28489378094673157 accuracy: 87.5\n",
      "1630 loss: 0.4471530616283417 accuracy: 87.5\n",
      "1640 loss: 0.26672086119651794 accuracy: 90.625\n",
      "1650 loss: 0.38899242877960205 accuracy: 81.25\n",
      "1660 loss: 0.3641672432422638 accuracy: 87.5\n",
      "1670 loss: 0.465898334980011 accuracy: 78.125\n",
      "1680 loss: 0.20262259244918823 accuracy: 93.75\n",
      "1690 loss: 0.2746659219264984 accuracy: 90.625\n",
      "1700 loss: 0.44178158044815063 accuracy: 78.125\n",
      "1710 loss: 0.24978432059288025 accuracy: 90.625\n",
      "1720 loss: 0.3472077548503876 accuracy: 87.5\n",
      "1730 loss: 0.45783308148384094 accuracy: 84.375\n",
      "1740 loss: 0.14538367092609406 accuracy: 93.75\n",
      "1750 loss: 0.25430744886398315 accuracy: 90.625\n",
      "1760 loss: 0.27452313899993896 accuracy: 84.375\n",
      "1770 loss: 0.4332481920719147 accuracy: 87.5\n",
      "1780 loss: 0.09878841787576675 accuracy: 96.875\n",
      "1790 loss: 0.5134702920913696 accuracy: 78.125\n",
      "1800 loss: 0.3290726840496063 accuracy: 81.25\n",
      "1810 loss: 0.2484598606824875 accuracy: 87.5\n",
      "1820 loss: 0.25522780418395996 accuracy: 90.625\n",
      "1830 loss: 0.21668776869773865 accuracy: 90.625\n",
      "1840 loss: 0.12505733966827393 accuracy: 100.0\n",
      "1850 loss: 0.39740222692489624 accuracy: 84.375\n",
      "1860 loss: 0.20953083038330078 accuracy: 93.75\n",
      "1870 loss: 0.16673415899276733 accuracy: 93.75\n",
      "0 loss: 0.14463073015213013 accuracy: 93.75\n",
      "10 loss: 0.28084659576416016 accuracy: 87.5\n",
      "20 loss: 0.44058507680892944 accuracy: 81.25\n",
      "30 loss: 0.3019861578941345 accuracy: 84.375\n",
      "40 loss: 0.17635460197925568 accuracy: 90.625\n",
      "50 loss: 0.3374454975128174 accuracy: 87.5\n",
      "60 loss: 0.17852124571800232 accuracy: 93.75\n",
      "70 loss: 0.37507104873657227 accuracy: 87.5\n",
      "80 loss: 0.4267931580543518 accuracy: 87.5\n",
      "90 loss: 0.3881601393222809 accuracy: 87.5\n",
      "100 loss: 0.1520250290632248 accuracy: 93.75\n",
      "110 loss: 0.3512352705001831 accuracy: 87.5\n",
      "120 loss: 0.2914266586303711 accuracy: 84.375\n",
      "130 loss: 0.19164003431797028 accuracy: 96.875\n",
      "140 loss: 0.4457738399505615 accuracy: 81.25\n",
      "150 loss: 0.08057644218206406 accuracy: 96.875\n",
      "160 loss: 0.2557843029499054 accuracy: 87.5\n",
      "170 loss: 0.7570672631263733 accuracy: 84.375\n",
      "180 loss: 0.23445861041545868 accuracy: 93.75\n",
      "190 loss: 0.08938129246234894 accuracy: 96.875\n",
      "200 loss: 0.20771680772304535 accuracy: 93.75\n",
      "210 loss: 0.21583455801010132 accuracy: 90.625\n",
      "220 loss: 0.566851794719696 accuracy: 81.25\n",
      "230 loss: 0.5140318274497986 accuracy: 81.25\n",
      "240 loss: 0.26144447922706604 accuracy: 93.75\n",
      "250 loss: 0.16251268982887268 accuracy: 96.875\n",
      "260 loss: 0.3778572976589203 accuracy: 81.25\n",
      "270 loss: 0.25760117173194885 accuracy: 87.5\n",
      "280 loss: 0.3549017906188965 accuracy: 87.5\n",
      "290 loss: 0.24821580946445465 accuracy: 90.625\n",
      "300 loss: 0.44851139187812805 accuracy: 78.125\n",
      "310 loss: 0.6850528120994568 accuracy: 84.375\n",
      "320 loss: 0.20345301926136017 accuracy: 93.75\n",
      "330 loss: 0.2375708520412445 accuracy: 93.75\n",
      "340 loss: 0.3928898274898529 accuracy: 81.25\n",
      "350 loss: 0.15368770062923431 accuracy: 96.875\n",
      "360 loss: 0.43706849217414856 accuracy: 81.25\n",
      "370 loss: 0.14450685679912567 accuracy: 93.75\n",
      "380 loss: 0.22065888345241547 accuracy: 87.5\n",
      "390 loss: 0.29763948917388916 accuracy: 90.625\n",
      "400 loss: 0.3216220438480377 accuracy: 87.5\n",
      "410 loss: 0.20802456140518188 accuracy: 93.75\n",
      "420 loss: 0.5235750079154968 accuracy: 84.375\n",
      "430 loss: 0.3779388666152954 accuracy: 87.5\n",
      "440 loss: 0.26376327872276306 accuracy: 93.75\n",
      "450 loss: 0.44475069642066956 accuracy: 87.5\n",
      "460 loss: 0.4462292790412903 accuracy: 87.5\n",
      "470 loss: 0.1901191771030426 accuracy: 93.75\n",
      "480 loss: 0.18709349632263184 accuracy: 90.625\n",
      "490 loss: 0.5661119818687439 accuracy: 84.375\n",
      "500 loss: 0.5577231645584106 accuracy: 78.125\n",
      "510 loss: 0.3663666546344757 accuracy: 87.5\n",
      "520 loss: 0.25752654671669006 accuracy: 87.5\n",
      "530 loss: 0.6265314817428589 accuracy: 78.125\n",
      "540 loss: 0.49290502071380615 accuracy: 84.375\n",
      "550 loss: 0.3455080986022949 accuracy: 81.25\n",
      "560 loss: 0.11382761597633362 accuracy: 96.875\n",
      "570 loss: 0.18838536739349365 accuracy: 93.75\n",
      "580 loss: 0.37830764055252075 accuracy: 87.5\n",
      "590 loss: 0.4129200577735901 accuracy: 84.375\n",
      "600 loss: 0.3593266010284424 accuracy: 87.5\n",
      "610 loss: 0.33644628524780273 accuracy: 87.5\n",
      "620 loss: 0.4923269748687744 accuracy: 78.125\n",
      "630 loss: 0.2878032624721527 accuracy: 87.5\n",
      "640 loss: 0.20144173502922058 accuracy: 90.625\n",
      "650 loss: 0.1650821715593338 accuracy: 96.875\n",
      "660 loss: 0.3672749996185303 accuracy: 87.5\n",
      "670 loss: 0.36414918303489685 accuracy: 90.625\n",
      "680 loss: 0.30276918411254883 accuracy: 90.625\n",
      "690 loss: 0.15644848346710205 accuracy: 93.75\n",
      "700 loss: 0.34285005927085876 accuracy: 81.25\n",
      "710 loss: 0.41644471883773804 accuracy: 87.5\n",
      "720 loss: 0.17076946794986725 accuracy: 90.625\n",
      "730 loss: 0.21504361927509308 accuracy: 93.75\n",
      "740 loss: 0.18343286216259003 accuracy: 93.75\n",
      "750 loss: 0.46362560987472534 accuracy: 84.375\n",
      "760 loss: 0.27620986104011536 accuracy: 90.625\n",
      "770 loss: 0.45110583305358887 accuracy: 87.5\n",
      "780 loss: 0.3355206847190857 accuracy: 87.5\n",
      "790 loss: 0.5579719543457031 accuracy: 81.25\n",
      "800 loss: 0.628264307975769 accuracy: 84.375\n",
      "810 loss: 0.5446773767471313 accuracy: 78.125\n",
      "820 loss: 0.26109403371810913 accuracy: 93.75\n",
      "830 loss: 0.1554577797651291 accuracy: 90.625\n",
      "840 loss: 0.4066484570503235 accuracy: 84.375\n",
      "850 loss: 0.7806339859962463 accuracy: 84.375\n",
      "860 loss: 0.260913223028183 accuracy: 87.5\n",
      "870 loss: 0.4220590889453888 accuracy: 87.5\n",
      "880 loss: 0.21369723975658417 accuracy: 93.75\n",
      "890 loss: 0.5450602173805237 accuracy: 78.125\n",
      "900 loss: 0.0647134780883789 accuracy: 96.875\n",
      "910 loss: 0.6011757254600525 accuracy: 78.125\n",
      "920 loss: 0.11673994362354279 accuracy: 100.0\n",
      "930 loss: 0.3043953776359558 accuracy: 87.5\n",
      "940 loss: 0.24102994799613953 accuracy: 90.625\n",
      "950 loss: 0.2664213180541992 accuracy: 87.5\n",
      "960 loss: 0.18685440719127655 accuracy: 90.625\n",
      "970 loss: 0.17677131295204163 accuracy: 93.75\n",
      "980 loss: 0.5590664148330688 accuracy: 81.25\n",
      "990 loss: 0.26929840445518494 accuracy: 87.5\n",
      "1000 loss: 0.4992305636405945 accuracy: 84.375\n",
      "1010 loss: 0.21519875526428223 accuracy: 93.75\n",
      "1020 loss: 0.2519645392894745 accuracy: 96.875\n",
      "1030 loss: 0.40171870589256287 accuracy: 90.625\n",
      "1040 loss: 0.08905968070030212 accuracy: 93.75\n",
      "1050 loss: 0.3068787157535553 accuracy: 90.625\n",
      "1060 loss: 0.5596446394920349 accuracy: 81.25\n",
      "1070 loss: 0.22133947908878326 accuracy: 87.5\n",
      "1080 loss: 0.20677126944065094 accuracy: 90.625\n",
      "1090 loss: 0.288507878780365 accuracy: 90.625\n",
      "1100 loss: 0.34327250719070435 accuracy: 87.5\n",
      "1110 loss: 0.2368033528327942 accuracy: 90.625\n",
      "1120 loss: 0.22075782716274261 accuracy: 90.625\n",
      "1130 loss: 0.3149295449256897 accuracy: 90.625\n",
      "1140 loss: 0.3142702877521515 accuracy: 90.625\n",
      "1150 loss: 0.2656061053276062 accuracy: 87.5\n",
      "1160 loss: 0.25697019696235657 accuracy: 90.625\n",
      "1170 loss: 0.5657702088356018 accuracy: 81.25\n",
      "1180 loss: 0.3037842810153961 accuracy: 93.75\n",
      "1190 loss: 0.2026997059583664 accuracy: 90.625\n",
      "1200 loss: 0.2582278847694397 accuracy: 84.375\n",
      "1210 loss: 0.43079066276550293 accuracy: 81.25\n",
      "1220 loss: 0.3143625259399414 accuracy: 84.375\n",
      "1230 loss: 0.5442697405815125 accuracy: 84.375\n",
      "1240 loss: 0.2890607714653015 accuracy: 90.625\n",
      "1250 loss: 0.313853919506073 accuracy: 84.375\n",
      "1260 loss: 0.13259154558181763 accuracy: 96.875\n",
      "1270 loss: 0.22975033521652222 accuracy: 96.875\n",
      "1280 loss: 0.3181551694869995 accuracy: 90.625\n",
      "1290 loss: 0.4095001816749573 accuracy: 81.25\n",
      "1300 loss: 0.2350378781557083 accuracy: 90.625\n",
      "1310 loss: 0.2922612428665161 accuracy: 87.5\n",
      "1320 loss: 0.07358136773109436 accuracy: 100.0\n",
      "1330 loss: 0.45327386260032654 accuracy: 84.375\n",
      "1340 loss: 0.670849621295929 accuracy: 90.625\n",
      "1350 loss: 0.4427332282066345 accuracy: 84.375\n",
      "1360 loss: 0.23507942259311676 accuracy: 90.625\n",
      "1370 loss: 0.20863313972949982 accuracy: 93.75\n",
      "1380 loss: 0.35191336274147034 accuracy: 84.375\n",
      "1390 loss: 0.20318235456943512 accuracy: 90.625\n",
      "1400 loss: 0.26111528277397156 accuracy: 87.5\n",
      "1410 loss: 0.15787282586097717 accuracy: 90.625\n",
      "1420 loss: 0.2131635546684265 accuracy: 90.625\n",
      "1430 loss: 0.19165512919425964 accuracy: 90.625\n",
      "1440 loss: 0.4162730276584625 accuracy: 90.625\n",
      "1450 loss: 0.38061392307281494 accuracy: 84.375\n",
      "1460 loss: 0.18402239680290222 accuracy: 90.625\n",
      "1470 loss: 0.34533998370170593 accuracy: 81.25\n",
      "1480 loss: 0.2328219711780548 accuracy: 90.625\n",
      "1490 loss: 0.12120289355516434 accuracy: 96.875\n",
      "1500 loss: 0.2946642339229584 accuracy: 87.5\n",
      "1510 loss: 0.4800969958305359 accuracy: 81.25\n",
      "1520 loss: 0.3404451012611389 accuracy: 93.75\n",
      "1530 loss: 0.2356232851743698 accuracy: 87.5\n",
      "1540 loss: 0.4149470329284668 accuracy: 87.5\n",
      "1550 loss: 0.13874541223049164 accuracy: 93.75\n",
      "1560 loss: 0.17342695593833923 accuracy: 93.75\n",
      "1570 loss: 0.19209139049053192 accuracy: 96.875\n",
      "1580 loss: 0.38105618953704834 accuracy: 90.625\n",
      "1590 loss: 0.26407390832901 accuracy: 90.625\n",
      "1600 loss: 0.2216145098209381 accuracy: 87.5\n",
      "1610 loss: 0.19696113467216492 accuracy: 90.625\n",
      "1620 loss: 0.29013845324516296 accuracy: 84.375\n",
      "1630 loss: 0.280377060174942 accuracy: 93.75\n",
      "1640 loss: 0.4884769022464752 accuracy: 78.125\n",
      "1650 loss: 0.29214248061180115 accuracy: 90.625\n",
      "1660 loss: 0.5717069506645203 accuracy: 81.25\n",
      "1670 loss: 0.40806323289871216 accuracy: 84.375\n",
      "1680 loss: 0.20921136438846588 accuracy: 93.75\n",
      "1690 loss: 0.14410048723220825 accuracy: 96.875\n",
      "1700 loss: 0.3046633303165436 accuracy: 87.5\n",
      "1710 loss: 0.15434768795967102 accuracy: 96.875\n",
      "1720 loss: 0.3354424834251404 accuracy: 87.5\n",
      "1730 loss: 0.25347208976745605 accuracy: 93.75\n",
      "1740 loss: 0.24961532652378082 accuracy: 93.75\n",
      "1750 loss: 0.264775812625885 accuracy: 90.625\n",
      "1760 loss: 0.2799731492996216 accuracy: 84.375\n",
      "1770 loss: 0.37574848532676697 accuracy: 87.5\n",
      "1780 loss: 0.2862164378166199 accuracy: 90.625\n",
      "1790 loss: 0.1587846428155899 accuracy: 93.75\n",
      "1800 loss: 0.16980059444904327 accuracy: 93.75\n",
      "1810 loss: 0.2974631190299988 accuracy: 90.625\n",
      "1820 loss: 0.3031547963619232 accuracy: 87.5\n",
      "1830 loss: 0.2825723886489868 accuracy: 93.75\n",
      "1840 loss: 0.212155282497406 accuracy: 96.875\n",
      "1850 loss: 0.25676819682121277 accuracy: 90.625\n",
      "1860 loss: 0.19795237481594086 accuracy: 87.5\n",
      "1870 loss: 0.22018350660800934 accuracy: 90.625\n",
      "0 loss: 0.14078009128570557 accuracy: 93.75\n",
      "10 loss: 0.28339454531669617 accuracy: 87.5\n",
      "20 loss: 0.2584793269634247 accuracy: 93.75\n",
      "30 loss: 0.1942726969718933 accuracy: 93.75\n",
      "40 loss: 0.28242775797843933 accuracy: 93.75\n",
      "50 loss: 0.2194920778274536 accuracy: 93.75\n",
      "60 loss: 0.42035776376724243 accuracy: 87.5\n",
      "70 loss: 0.24414977431297302 accuracy: 90.625\n",
      "80 loss: 0.34149840474128723 accuracy: 90.625\n",
      "90 loss: 0.24466294050216675 accuracy: 93.75\n",
      "100 loss: 0.11708923429250717 accuracy: 93.75\n",
      "110 loss: 0.19462019205093384 accuracy: 96.875\n",
      "120 loss: 0.2015361487865448 accuracy: 90.625\n",
      "130 loss: 0.252542108297348 accuracy: 93.75\n",
      "140 loss: 0.08943841606378555 accuracy: 100.0\n",
      "150 loss: 0.23937830328941345 accuracy: 93.75\n",
      "160 loss: 0.18526197969913483 accuracy: 93.75\n",
      "170 loss: 0.34869158267974854 accuracy: 87.5\n",
      "180 loss: 0.19379280507564545 accuracy: 87.5\n",
      "190 loss: 0.34267958998680115 accuracy: 87.5\n",
      "200 loss: 0.37667715549468994 accuracy: 93.75\n",
      "210 loss: 0.23537109792232513 accuracy: 87.5\n",
      "220 loss: 0.25525426864624023 accuracy: 90.625\n",
      "230 loss: 0.23024402558803558 accuracy: 87.5\n",
      "240 loss: 0.28300419449806213 accuracy: 87.5\n",
      "250 loss: 0.2358609139919281 accuracy: 90.625\n",
      "260 loss: 0.37712085247039795 accuracy: 87.5\n",
      "270 loss: 0.1857251077890396 accuracy: 87.5\n",
      "280 loss: 0.2859377861022949 accuracy: 87.5\n",
      "290 loss: 0.38990887999534607 accuracy: 87.5\n",
      "300 loss: 0.1347533017396927 accuracy: 93.75\n",
      "310 loss: 0.24482373893260956 accuracy: 93.75\n",
      "320 loss: 0.4112495183944702 accuracy: 87.5\n",
      "330 loss: 0.2021479606628418 accuracy: 96.875\n",
      "340 loss: 0.30417925119400024 accuracy: 87.5\n",
      "350 loss: 0.534697413444519 accuracy: 81.25\n",
      "360 loss: 0.4022904336452484 accuracy: 78.125\n",
      "370 loss: 0.4685714542865753 accuracy: 84.375\n",
      "380 loss: 0.13174192607402802 accuracy: 93.75\n",
      "390 loss: 0.19307541847229004 accuracy: 90.625\n",
      "400 loss: 0.21309514343738556 accuracy: 90.625\n",
      "410 loss: 0.26955440640449524 accuracy: 93.75\n",
      "420 loss: 0.40155649185180664 accuracy: 84.375\n",
      "430 loss: 0.28167515993118286 accuracy: 90.625\n",
      "440 loss: 0.24294880032539368 accuracy: 90.625\n",
      "450 loss: 0.24151501059532166 accuracy: 90.625\n",
      "460 loss: 0.403255432844162 accuracy: 81.25\n",
      "470 loss: 0.287451833486557 accuracy: 87.5\n",
      "480 loss: 0.32371804118156433 accuracy: 87.5\n",
      "490 loss: 0.14940880239009857 accuracy: 90.625\n",
      "500 loss: 0.310303270816803 accuracy: 84.375\n",
      "510 loss: 0.5278533101081848 accuracy: 84.375\n",
      "520 loss: 0.3719038963317871 accuracy: 81.25\n",
      "530 loss: 0.4367472231388092 accuracy: 84.375\n",
      "540 loss: 0.1590796411037445 accuracy: 93.75\n",
      "550 loss: 0.3510178327560425 accuracy: 90.625\n",
      "560 loss: 0.3484727144241333 accuracy: 81.25\n",
      "570 loss: 0.07209277153015137 accuracy: 96.875\n",
      "580 loss: 0.11712391674518585 accuracy: 96.875\n",
      "590 loss: 0.17344017326831818 accuracy: 90.625\n",
      "600 loss: 0.143978551030159 accuracy: 93.75\n",
      "610 loss: 0.13193251192569733 accuracy: 96.875\n",
      "620 loss: 0.5873616933822632 accuracy: 84.375\n",
      "630 loss: 0.3465979993343353 accuracy: 90.625\n",
      "640 loss: 0.20120801031589508 accuracy: 93.75\n",
      "650 loss: 0.29104378819465637 accuracy: 87.5\n",
      "660 loss: 0.1596752405166626 accuracy: 93.75\n",
      "670 loss: 0.21114583313465118 accuracy: 90.625\n",
      "680 loss: 0.3416934311389923 accuracy: 81.25\n",
      "690 loss: 0.8802900314331055 accuracy: 78.125\n",
      "700 loss: 0.25542429089546204 accuracy: 93.75\n",
      "710 loss: 0.33067330718040466 accuracy: 90.625\n",
      "720 loss: 0.28012311458587646 accuracy: 90.625\n",
      "730 loss: 0.5653846263885498 accuracy: 78.125\n",
      "740 loss: 0.17424392700195312 accuracy: 96.875\n",
      "750 loss: 0.43773284554481506 accuracy: 81.25\n",
      "760 loss: 0.2237538844347 accuracy: 87.5\n",
      "770 loss: 0.10921518504619598 accuracy: 100.0\n",
      "780 loss: 0.6442469358444214 accuracy: 81.25\n",
      "790 loss: 0.25508344173431396 accuracy: 93.75\n",
      "800 loss: 0.17888030409812927 accuracy: 93.75\n",
      "810 loss: 0.26330363750457764 accuracy: 93.75\n",
      "820 loss: 0.2867870330810547 accuracy: 90.625\n",
      "830 loss: 0.2779112458229065 accuracy: 93.75\n",
      "840 loss: 0.35348036885261536 accuracy: 84.375\n",
      "850 loss: 0.20520475506782532 accuracy: 93.75\n",
      "860 loss: 0.2806292772293091 accuracy: 90.625\n",
      "870 loss: 0.2547983229160309 accuracy: 87.5\n",
      "880 loss: 0.18768468499183655 accuracy: 93.75\n",
      "890 loss: 0.1229991614818573 accuracy: 93.75\n",
      "900 loss: 0.3230831027030945 accuracy: 87.5\n",
      "910 loss: 0.23489192128181458 accuracy: 93.75\n",
      "920 loss: 0.2849459648132324 accuracy: 93.75\n",
      "930 loss: 0.41168004274368286 accuracy: 84.375\n",
      "940 loss: 0.47105222940444946 accuracy: 90.625\n",
      "950 loss: 0.2764838635921478 accuracy: 90.625\n"
     ]
    }
   ],
   "source": [
    "# 进行9次迭代\n",
    "for _ in range(3):\n",
    "    # 遍历数据批次\n",
    "    for n_, batch in enumerate(train_batch):\n",
    "        # 将输入数据X调整形状并输入到模型\n",
    "        X = batch[0].to(device)\n",
    "        # y为真实标签\n",
    "        y = batch[1].to(device)\n",
    "\n",
    "        # 前向传播，获取模型输出\n",
    "        sigma = model.forward(X)\n",
    "        # 计算损失\n",
    "        loss = criterion(sigma, y)\n",
    "        # 计算预测的标签\n",
    "        y_hat = torch.max(sigma, dim=1)[1]\n",
    "        # 计算预测正确的数量\n",
    "        correct_count = torch.sum(y_hat == y)\n",
    "        # 计算准确率\n",
    "        accuracy = correct_count / len(y) * 100\n",
    "        # 反向传播，计算梯度\n",
    "        loss.backward()\n",
    "        # 更新模型参数\n",
    "        opt.step()\n",
    "        # 清除之前的梯度\n",
    "        model.zero_grad()\n",
    "        if n_ % 10 == 0:\n",
    "            # 打印当前批次的损失和准确率\n",
    "            print(n_, 'loss:', loss.item(), 'accuracy:', accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c65c68-392f-4a91-a5ce-0d8c7b07916b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correct_count = 0\n",
    "for batch in test_batch:\n",
    "    test_X = batch[0].to(device)\n",
    "    test_y = batch[1].to(device)\n",
    "    sigma = model.forward(torch.tensor(test_X, dtype=torch.float32))\n",
    "    y_hat = torch.max(sigma, dim=1)[1]\n",
    "    correct_count += torch.sum(y_hat == test_y)\n",
    "    \n",
    "accuracy = correct_count / 10000 * 100\n",
    "print('accuracy:', accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eb2d31-5255-48b5-b2aa-8fbdf8279277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

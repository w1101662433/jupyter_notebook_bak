{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Viewer.__del__ at 0x000002667BA29620>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\python\\Python311\\Lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 185, in __del__\n",
      "    self.close()\n",
      "  File \"D:\\python\\Python311\\Lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 101, in close\n",
      "    self.window.close()\n",
      "  File \"D:\\python\\Python311\\Lib\\site-packages\\pyglet\\window\\win32\\__init__.py\", line 313, in close\n",
      "    super(Win32Window, self).close()\n",
      "  File \"D:\\python\\Python311\\Lib\\site-packages\\pyglet\\window\\__init__.py\", line 838, in close\n",
      "    app.windows.remove(self)\n",
      "  File \"D:\\python\\Python311\\Lib\\_weakrefset.py\", line 113, in remove\n",
      "    self.data.remove(ref(item))\n",
      "KeyError: <weakref at 0x0000026606EAD1C0; to 'Win32Window' at 0x0000026608DC9B10>\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#创建环境\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "\n",
    "#打印游戏\n",
    "def show():\n",
    "    env.render(mode='rgb_array')\n",
    "\n",
    "\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个游戏的状态用4个数字表示,我也不知道这4个数字分别是什么意思,反正这4个数字就能描述游戏全部的状态\n",
      "state= [ 0.03645723 -0.00906221  0.03182828 -0.03750419]\n",
      "这个游戏一共有2个动作,不是0就是1\n",
      "env.action_space= Discrete(2)\n",
      "随机一个动作\n",
      "action= 0\n",
      "执行一个动作,得到下一个状态,奖励,是否结束\n",
      "state= [ 0.03627598 -0.20462578  0.03107819  0.26504853]\n",
      "reward= 1.0\n",
      "over= False\n"
     ]
    }
   ],
   "source": [
    "#测试游戏环境\n",
    "def test_env():\n",
    "    state = env.reset()\n",
    "    print('这个游戏的状态用4个数字表示,我也不知道这4个数字分别是什么意思,反正这4个数字就能描述游戏全部的状态')\n",
    "    print('state=', state)\n",
    "    #state= [ 0.03490619  0.04873464  0.04908862 -0.00375859]\n",
    "\n",
    "    print('这个游戏一共有2个动作,不是0就是1')\n",
    "    print('env.action_space=', env.action_space)\n",
    "    #env.action_space= Discrete(2)\n",
    "\n",
    "    print('随机一个动作')\n",
    "    action = env.action_space.sample()\n",
    "    print('action=', action)\n",
    "    #action= 1\n",
    "\n",
    "    print('执行一个动作,得到下一个状态,奖励,是否结束')\n",
    "    state, reward, over, _ = env.step(action)\n",
    "\n",
    "    print('state=', state)\n",
    "    #state= [ 0.02018229 -0.16441101  0.01547085  0.2661691 ]\n",
    "\n",
    "    print('reward=', reward)\n",
    "    #reward= 1.0\n",
    "\n",
    "    print('over=', over)\n",
    "    #over= False\n",
    "\n",
    "\n",
    "test_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=16, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#计算动作的模型,也是真正要用的模型\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 16),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(16, 2),\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "#得到一个动作\n",
    "def get_action(state):\n",
    "    if random.random() < 0.01:\n",
    "        return random.choice([0, 1])\n",
    "\n",
    "    #走神经网络,得到一个动作\n",
    "    state = torch.FloatTensor(state).reshape(1, 4)\n",
    "\n",
    "    return model(state).argmax().item()\n",
    "\n",
    "\n",
    "get_action([0.0013847, -0.01194451, 0.04260966, 0.00688801])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((203, 0), 203)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#样本池\n",
    "datas = []\n",
    "\n",
    "\n",
    "#向样本池中添加N条数据,删除M条最古老的数据\n",
    "def update_data():\n",
    "    old_count = len(datas)\n",
    "\n",
    "    #玩到新增了N个数据为止\n",
    "    while len(datas) - old_count < 200:\n",
    "        #初始化游戏\n",
    "        state = env.reset()\n",
    "\n",
    "        #玩到游戏结束为止\n",
    "        over = False\n",
    "        while not over:\n",
    "            #根据当前状态得到一个动作\n",
    "            action = get_action(state)\n",
    "\n",
    "            #执行动作,得到反馈\n",
    "            next_state, reward, over, _ = env.step(action)\n",
    "\n",
    "            #记录数据样本\n",
    "            datas.append((state, action, reward, next_state, over))\n",
    "\n",
    "            #更新游戏状态,开始下一个动作\n",
    "            state = next_state\n",
    "\n",
    "    update_count = len(datas) - old_count\n",
    "    drop_count = max(len(datas) - 10000, 0)\n",
    "\n",
    "    #数据上限,超出时从最古老的开始删除\n",
    "    while len(datas) > 10000:\n",
    "        datas.pop(0)\n",
    "\n",
    "    return update_count, drop_count\n",
    "\n",
    "\n",
    "update_data(), len(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取一批数据样本\n",
    "def get_sample():\n",
    "    #从样本池中采样\n",
    "    samples = random.sample(datas, 64)\n",
    "\n",
    "    #[b, 4]\n",
    "    state = torch.FloatTensor([i[0] for i in samples])\n",
    "    #[b]\n",
    "    action = torch.LongTensor([i[1] for i in samples])\n",
    "    #[b]\n",
    "    reward = torch.FloatTensor([i[2] for i in samples])\n",
    "    #[b, 4]\n",
    "    next_state = torch.FloatTensor([i[3] for i in samples])\n",
    "    #[b]\n",
    "    over = torch.LongTensor([i[4] for i in samples])\n",
    "\n",
    "    return state, action, reward, next_state, over\n",
    "\n",
    "\n",
    "state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "# state[:5], action, reward, next_state[:5], over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0611,  0.0598,  0.0705, -0.0352,  0.0931, -0.0755, -0.1808,  0.1001,\n",
       "        -0.0226,  0.0819,  0.0828, -0.0688, -0.1497,  0.0818, -0.0485, -0.0841,\n",
       "         0.0994, -0.0369, -0.0736,  0.0889, -0.1180,  0.0871,  0.0607,  0.0747,\n",
       "         0.0045, -0.1385, -0.1587,  0.0126, -0.1026,  0.0888,  0.0531, -0.1079,\n",
       "        -0.0960,  0.0347,  0.0905, -0.1280,  0.0948,  0.0189,  0.0405,  0.0786,\n",
       "        -0.1283,  0.0735, -0.1047, -0.0329, -0.0849, -0.1369, -0.1626,  0.0135,\n",
       "         0.0689, -0.1424,  0.0856,  0.0237, -0.1179,  0.0742, -0.0809, -0.1855,\n",
       "         0.0656, -0.0314, -0.0563,  0.0945, -0.0850,  0.0781,  0.0621,  0.0407],\n",
       "       grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_value(state, action):\n",
    "    #使用状态计算出动作的logits\n",
    "    #[b, 4] -> [b, 2]\n",
    "    value = model(state)\n",
    "\n",
    "    #根据实际使用的action取出每一个值\n",
    "    #这个值就是模型评估的在该状态下,执行动作的分数\n",
    "    #在执行动作前,显然并不知道会得到的反馈和next_state\n",
    "    #所以这里不能也不需要考虑next_state和reward\n",
    "    #[b, 2] -> [b]\n",
    "    value = value[range(64), action]\n",
    "\n",
    "    return value\n",
    "\n",
    "\n",
    "get_value(state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9037, 1.0551, 1.0642, 0.9288, 1.0448, 0.8897, 1.0000, 1.0960, 0.9419,\n",
       "        1.0761, 1.0763, 0.8994, 0.8190, 1.0770, 0.9208, 0.8818, 1.0959, 0.9260,\n",
       "        0.8943, 1.0848, 0.8551, 1.0826, 1.0552, 1.0695, 0.9499, 1.0000, 0.8145,\n",
       "        0.9774, 0.8694, 1.0832, 1.0477, 0.8642, 0.8742, 0.9879, 1.0841, 0.8407,\n",
       "        1.0908, 0.9639, 0.9905, 1.0399, 1.0000, 1.0657, 1.0000, 0.9335, 0.8844,\n",
       "        1.0000, 1.0000, 0.9645, 1.0629, 0.8287, 1.0814, 0.9747, 0.8486, 1.0689,\n",
       "        0.8863, 1.0000, 1.0132, 0.9326, 0.9135, 1.0889, 0.8816, 1.0742, 1.0185,\n",
       "        0.9885])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_target(reward, next_state, over):\n",
    "    #上面已经把模型认为的状态下执行动作的分数给评估出来了\n",
    "    #下面使用next_state和reward计算真实的分数\n",
    "    #针对一个状态,它到底应该多少分,可以使用以往模型积累的经验评估\n",
    "    #这也是没办法的办法,因为显然没有精确解,这里使用延迟更新的next_model评估\n",
    "\n",
    "    #使用next_state计算下一个状态的分数\n",
    "    #[b, 4] -> [b, 2]\n",
    "    with torch.no_grad():\n",
    "        target = model(next_state)\n",
    "\n",
    "    #取所有动作中分数最大的\n",
    "    #[b, 2] -> [b]\n",
    "    target = target.max(dim=1)[0]\n",
    "\n",
    "    #如果next_state已经游戏结束,则next_state的分数是0\n",
    "    #因为如果下一步已经游戏结束,显然不需要再继续玩下去,也就不需要考虑next_state了.\n",
    "    #[b]\n",
    "    for i in range(64):\n",
    "        if over[i]:\n",
    "            target[i] = 0\n",
    "\n",
    "    #下一个状态的分数乘以一个系数,相当于权重\n",
    "    #[b] * [b] -> [b]\n",
    "    target *= 0.98\n",
    "\n",
    "    #加上reward就是最终的分数\n",
    "    #[b] + [b] -> [b]\n",
    "    target += reward\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "get_target(reward, next_state, over)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "\n",
    "\n",
    "def test(play):\n",
    "    #初始化游戏\n",
    "    state = env.reset()\n",
    "\n",
    "    #记录反馈值的和,这个值越大越好\n",
    "    reward_sum = 0\n",
    "\n",
    "    #玩到游戏结束为止\n",
    "    over = False\n",
    "    while not over:\n",
    "        #根据当前状态得到一个动作\n",
    "        action = get_action(state)\n",
    "\n",
    "        #执行动作,得到反馈\n",
    "        state, reward, over, _ = env.step(action)\n",
    "        reward_sum += reward\n",
    "\n",
    "        #打印动画\n",
    "        if play:\n",
    "            display.clear_output(wait=True)\n",
    "            show()\n",
    "\n",
    "    return reward_sum\n",
    "\n",
    "\n",
    "test(play=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "OHoSU6uI-xIt",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 408 205 0 9.2\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    #训练N次\n",
    "    for epoch in range(1):\n",
    "        #更新N条数据\n",
    "        update_count, drop_count = update_data()\n",
    "\n",
    "        #每次更新过数据后,学习N次\n",
    "        for i in range(1):\n",
    "            #采样一批数据\n",
    "            state, action, reward, next_state, over = get_sample()\n",
    "\n",
    "            #计算一批样本的value和target\n",
    "            value = get_value(state, action)\n",
    "            target = get_target(reward, next_state, over)\n",
    "\n",
    "            #更新参数\n",
    "            loss = loss_fn(value, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            test_result = sum([test(play=False) for _ in range(20)]) / 20\n",
    "            print(epoch, len(datas), update_count, drop_count, test_result)\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(play=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0112,  0.1397, -0.2651,  0.1428],\n",
       "        [-0.2196, -0.3746, -0.2789, -0.0629],\n",
       "        [-0.1487, -0.3741, -0.2815,  0.2363],\n",
       "        [ 0.2737,  0.4999,  0.1023,  0.2207],\n",
       "        [-0.0982,  0.3085, -0.0156, -0.2932],\n",
       "        [-0.4242,  0.3416,  0.2880, -0.3576],\n",
       "        [ 0.0163, -0.1218, -0.0212,  0.3527],\n",
       "        [-0.0578, -0.3019, -0.2483,  0.3652],\n",
       "        [-0.0698, -0.2277, -0.1791, -0.2455],\n",
       "        [-0.0932, -0.1505,  0.0902,  0.2408],\n",
       "        [-0.0397,  0.2067, -0.0609,  0.3886],\n",
       "        [-0.1679, -0.4822,  0.2225, -0.0483],\n",
       "        [ 0.0959, -0.1113,  0.0410,  0.1039],\n",
       "        [ 0.4980, -0.0071,  0.3389,  0.4030],\n",
       "        [ 0.0539, -0.3029,  0.4025, -0.1725],\n",
       "        [-0.2776,  0.1995, -0.3249,  0.4110]], requires_grad=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*model.parameters()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0092,  0.1417, -0.2671,  0.1408],\n",
       "        [-0.2176, -0.3726, -0.2809, -0.0649],\n",
       "        [-0.1467, -0.3721, -0.2835,  0.2343],\n",
       "        [ 0.2737,  0.4999,  0.1023,  0.2207],\n",
       "        [-0.0982,  0.3085, -0.0156, -0.2932],\n",
       "        [-0.4242,  0.3416,  0.2880, -0.3576],\n",
       "        [ 0.0143, -0.1238, -0.0192,  0.3547],\n",
       "        [-0.0558, -0.2999, -0.2503,  0.3632],\n",
       "        [-0.0678, -0.2257, -0.1811, -0.2475],\n",
       "        [-0.0952, -0.1525,  0.0922,  0.2428],\n",
       "        [-0.0377,  0.2087, -0.0629,  0.3866],\n",
       "        [-0.1659, -0.4802,  0.2205, -0.0503],\n",
       "        [ 0.0939, -0.1133,  0.0430,  0.1059],\n",
       "        [ 0.4960, -0.0091,  0.3409,  0.4050],\n",
       "        [ 0.0539, -0.3029,  0.4025, -0.1725],\n",
       "        [-0.2796,  0.1975, -0.3229,  0.4130]], requires_grad=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*model.parameters()][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "第7章-DQN算法.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
